{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 - ESTATÍSTICA BAYESIANA\n",
    "---\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidade Condicional \n",
    "\n",
    "\n",
    "Cálculo da probabilidade condicional de $P(C|X)$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Considere-se o problema seguinte:\n",
    "\n",
    "<img src=\"images/P_AaposB.png\" style=\"width:40%\"/>\n",
    "\n",
    "- três baldes de cor diferente $C \\in \\{Vermelho, Azul, Preto\\}$  \n",
    "- bolas com duas cores $ X \\in \\{Laranja, Verde\\}$ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Considere-se que $C$ e $X$ são variáveis aleatórias e $P(Laranja \\space|\\space Azul)$ corresponde a \n",
    "\n",
    "$$P(X = Laranja \\space|\\space C = Azul)$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "O que quer dizer probabilidade condicionada? Podemos Traduzir $P(X = Laranja \\space|\\space C = Azul)$ como \n",
    "\n",
    "> A probabilidade de encontrarmos uma bola $Laranja$, sabendo que estamos à procura no balde $Azul$ **(Probabilidade bola Laranja, dado balde Azul)**. Neste caso a probabilidade é dada por 1/4 (1 bola $Laranja$ num universo de 4 bolas).\n",
    "\n",
    "\n",
    "Repare-se que isto é diferente de dizer $P(X = Laranja , C = Azul)$ que quer dizer:\n",
    "\n",
    "> A probabilidade de encontrarmos uma bola $Laranja$ e um balde azul $Azul$ **(Probabilidade bola Laranja e balde Azul)**. Neste caso a probabilidade é dada por 1/14 (1 bola $Laranja$ num universo de 14 bolas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com esta Lógica, podemos preencher uma matriz (A) com todas as $P(C, X)$:\n",
    "\n",
    "|- |Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "\n",
    "Repare-se que se quisermos saber $P(C)$ (a probabiliade de retirarmos bolas de um dado balde), podemos somar as probabilidades da coluna dessa cor \n",
    "\n",
    "> $e.g.$,  Azul = 4/14, que pode ser retirada igualmente da imagem\n",
    "\n",
    "Esta regra escrita de forma formal significa que\n",
    "\n",
    "$$P(C) = \\sum_X P(C,X)$$\n",
    "\n",
    "Esta probabilidade chama-se de **probabilidade marginal** e trata-se da probabilidade de uma dada característica independentemente das outras.\n",
    "\n",
    "---\n",
    "\n",
    "Se a $P(X, C) = P(X)P(C)$, $X$ e $C$ dizem-se independentes. \n",
    "\n",
    "Neste caso dos baldes e das bolas não são: Seriam independentes se cada balde contivesse a mesma fração de bolas Laranjas e Verdes. Nesse caso $P(X|C) = P (X)$, de modo que a probabilidade de selecionar, digamos, uma bola verde era independente do balde escolhido e vice verse. \n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Probabilidade Condicionada\n",
    "----\n",
    "\n",
    "> Podem pensar na probabilidade condicionada como uma redução do universo em análise ao subconjunto condicionante. Quer isto dizer, que ao criarmos uma probabilidade condicionada estamos na verdade a limitar o número de bolas em análise.\n",
    "\n",
    "\n",
    "Neste caso podemos ter duas:\n",
    "\n",
    "- $P(X \\space| \\space C)$ - a probabilidade de obtermos uma dada cor da bola conhecendo o balde. \n",
    "\n",
    "\n",
    "- $P(C\\space| \\space X)$ - a probabilidade de obtermos um dado balde conhecendo a cor da bola. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tomemos a seguinte probabilidade: $P(Vermelho | Verde)$ que nos é dada pelas bolas do balde vermelho sabendo sabendo que estamos a analisar somente o subconjunto (condicionante) das bolas Verdes (que tem 7 bolas no total). O resultado é \n",
    "\n",
    "$$P(Vermelho|Verde) = 2/7$$\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**E se quisermos obter o valor da probabilidade condicionada com base nas restante probabilidades definidas anteriormente, ou seja, com base nas P(C,X), P(C) e P(X)?**\n",
    "\n",
    "\n",
    "\n",
    "|-|Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "> Considere-se para já este exercício: Olhando agora para a nossa tabela, podemos obter o valor 2/7 usando as probabilidades que definimos na tabela tipo $P(X,C)$ e as probabilidades marginais $P(X)$ ou $P(C)$?\n",
    "\n",
    "- podemos dizer que o 2 vem de $P(Vermelho,Verde) =2/14$ \n",
    "- e que o 7 vem da probabilidade de estarmos a tirar bolas Verdes, $P(Verde) = 7/14$. \n",
    "\n",
    "Assim, é facil de ver que \n",
    "\n",
    "$$P(Vermelho|Verde) = \\frac{P(Vermelho,Verde)}{P(Verde)}$$\n",
    "\n",
    "ou seja, de formalmente temos que \n",
    "\n",
    "$$P(X|C) = \\frac{P(X,C)}{P(C)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema de Bayes**\n",
    "\n",
    "\n",
    "Juntando a regra do produto com a propriedade $P(X,C) = P(C,X)$, obtemos:\n",
    "\n",
    "$$P(X,C) = P(X\\space|\\space C)P(C) <=> P(C,X) = P(C\\space|\\space X)P(X)$$\n",
    "\n",
    "ou seja, \n",
    "\n",
    "$$P(C\\space|\\space X)P(X) = P(X\\space|\\space C)P(C)$$\n",
    "\n",
    "e por isso (Teorema de Bayes):\n",
    "\n",
    "$$P(C\\space|\\space X) = \\frac{P(X\\space|\\space C)P(C)}{P(X)}$$\n",
    "\n",
    "Estas $P(X\\space|\\space C)$ e o $P(C)$ são retirados dos dados.\n",
    "\n",
    "<img src=\"images/bayesian_theorem.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onde nos leva o teorema de Bayes, no que diz respeito à aprendizagem automatica?\n",
    "\n",
    "---\n",
    "\n",
    "### Um problema Prático\n",
    "\n",
    "\n",
    "Vamos imaginar um cenário em que eu quero estimar a probabilidade do balde de origem de uma bola que tenho na minha mão (**problema de classificação em que a classe é o balde de origem e as features são as cores das bolas**) mas não sei a matriz A de frequências por balde e por cor porque **não tenho acesso aos dados todos ou porque são muitos (imaginem milhares de cores e milhares de baldes...)**\n",
    "\n",
    "> Por exemplo, queremos conhecer a probabilidade do balde ser Azul, sabendo que a cor da bola na minha mão é $Verde$ - $P(Azul \\space | \\space Verde)$?\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "Como resolver o problema? \n",
    "\n",
    " \n",
    "1) Também é facil de contar o número de cores de baldes $P(Azul)$\n",
    "\n",
    "2) **Posso fazer algumas experiências** - tirar algumas bolas de diversos baldes (por exemplo balde azul) e ver a probabilidade de sair uma cor. Isto quer dizer que conseguimos estimar $P(Verde \\space | \\space Azul)$.\n",
    "\n",
    "3) Da mesma forma, também posso contar a frequência marginal das cores das bolas que foram saindo ao longo da experiência $P(Verde)$\n",
    "\n",
    "\n",
    "\n",
    "**mas como podem estas estatísticas ajudar-nos a estimar** $P(Azul \\space | \\space Verde)$?\n",
    "\n",
    "\n",
    "Voltemos ao nosso modelo inicial:\n",
    "\n",
    "Só para percebermos o valor a que queríamos chegar, usando a matriz de frequencias A:\n",
    "\n",
    "|-|Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "$$P(Azul \\space | \\space Verde) = 3/7$$ \n",
    "\n",
    "\n",
    "$$P(Verde \\space | \\space Azul) = 3/4$$ (que neste caso é conhecido perfeitamente)\n",
    "\n",
    "\n",
    "(que vem de $P(X|Y) = \\frac{P(X,Y)}{P(Y)}$ ou se percebe por análise da matriz A)\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Portanto, queremos deduzir o valor de 3/7 a partir do conhecimento de 3/4: $$\\frac{3}{4} => \\frac{3}{7}$$ \n",
    "\n",
    "3 é o factor comum (quantidade de bolas) e 4 e 7 são o universo no qual estamos a definir cada uma das probabilidades, e é isso que queremos trocar.\n",
    "\n",
    "Como $P(Azul) = \\frac{4}{14}$ e $P(Verde) = \\frac{7}{14}$\n",
    "\n",
    "\n",
    "ao fazermos \n",
    "\n",
    "$$\\frac{P(Verde \\space | \\space Azul)P(Azul)}{P(Verde)} = \\frac{3}{7}$$ \n",
    "\n",
    "estamos a retirar a nossa base de existências do universo balde azul para o universo balde verde.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modelos de Classificação Bayesianos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consideremos dois cenários:\n",
    "\n",
    "1) **Prior Estimate** - Não temos mais informação senão o número de bolas total existentes em cada balde (sem saber cores);\n",
    "\n",
    "\n",
    "Neste primeiro cenario a melhor estimativa que temos é a chamada estimativa à priori (**Prior Estimate**):\n",
    "\n",
    "$$Max(P(Balde_j))$$\n",
    "\n",
    "\n",
    "\n",
    "> **Prior Estimate** - Só temos dados dados antes de tirarmos a bola, não temos dados após o evento de retirar a bola à posteriori, por isso se chama à priori. \n",
    "\n",
    "---\n",
    "\n",
    "2)  **Posterior Estimate** - sabemos também a cor da bola retirada e **são conhecidas as quantidades de cada cor das bolas em cada balde**\n",
    "\n",
    "> ou seja, consigo preencher uma matriz A com experiências e usar a lei de Bayes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos genericamente, para sabermos qual a origem mais provavel de uma bola, basta encontrar a probabilidade condicional maior:\n",
    "\n",
    "\n",
    "$$\\frac{P(Azul,Verde)}{P(Verde)} = \\frac{3/14}{7/14} = 1/7$$\n",
    "\n",
    "do balde Vermelho e Preto são:\n",
    "\n",
    "$$P(Vermelho| Verde)= 2/7 $$\n",
    "\n",
    "$$P(Preto| Verde)= 0/7 $$\n",
    "\n",
    "Portanto, a origem mais provável é o balde Vermelho, porque maximiza a probabilidade, tendo em conta os dados à priori (quantidades de bolas de cada cor em cada balde) e os dados à posteriori (cor da bola, ou seja, as features do novo registo a classificar).\n",
    "\n",
    "\n",
    "E estimativa que usámos chama-se **Maximum Posterior Estimate (MAP)** e permite maximizar a probabilidade tendo em conta os dados (features) do exemplar retirado.\n",
    "\n",
    "A **Maximum Posterior Estimate (MAP)** é dada por\n",
    "\n",
    "$$C_i: Max\\{P(C_i|X)\\}$$\n",
    "\n",
    "\n",
    "para $i =\\{1,...,N\\}$ onde $N$ é o número de baldes\n",
    "\n",
    "----\n",
    "\n",
    "O resultado do **MAP é considerado óptimo**, quando existem dados suficientes para o seu cálculo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na Verdade...\n",
    "\n",
    "Como vimos, Nem sempre existem dados suficientes que suportem o cálculo do MAP **porque a matriz de frequências A não é representativa da estatística**: por exemplo, ou porque não tenho acesso a esses dados todos ou porque são muitos mais uma vez, imaginem milhares de cores e milhares de baldes...)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo o que demos até aqui, é possível ver duas abordagens para definir a função $P(C_i|X)$ usada no MAP. E estas abordagens levam a métodos distintos, que defininem 2 grandes classes de classificadores:\n",
    "\n",
    "- **Algoritmos Discriminativos (Discriminative Algorithms)** - Que tentam aprender directamente o que é $P(C_i|X)$ e daí classificar novos registos. Exemplos são o logistic regression e as redes neuronais. Este modelo cria fronteiras de decisão entre as classes. Depois, em fase de classificação, o modelo verifica se o nosso modelo está de um lado ou de outro da fronteira de decisão e classifica de acordo com isso.\n",
    "\n",
    "\n",
    "- **Algoritmos Generativos (Generative Algorithms)** - Que tentam aprender através do teorema de Bayes, ou seja, dizendo que\n",
    "\n",
    "$$P(C\\space|\\space X) = \\frac{P(X\\space|\\space C)P(C)}{P(X)}$$\n",
    "\n",
    "Na fase de aprendizagem este tipo de modelos (probabilistico) criam uma definição sobre o que é cada classe $C_i$. Esta definição é dada por $P(X|C_i)P(C_i)$ \n",
    "\n",
    "> ou seja, quais as probabilidades de existir cada feature, dada uma classe.\n",
    "\n",
    "Na fase de classificação, ve qual o modelo que tem melhor score;\n",
    "O Naive Bayes que vamos estudar  é um método deste estilo. Vamos usar o MAP, aplicado à igualdade de Bayes, ou seja,\n",
    "\n",
    "$$C_i: Max\\{\\frac{P(X\\space|\\space C_i)P(C_i)}{P(X)}\\}$$\n",
    "\n",
    "Mas repare-se que a probabilidade de X não muda entre as diversas expressões, pelo que o máximo com o denominador é igual ao máximo sem ele:\n",
    "\n",
    "$$C_i: Max\\{P(X\\space|\\space C_i)P(C_i)\\}$$\n",
    "\n",
    "> E se tentássemos apenas maximizar $$C_i: Max\\{P(X\\space|\\space C_i)\\}$$ então estaríamos a aplicar o algoritmo maximum likelihood estimation (MLE) que é um caso especial do MAP, quando P(C_i) é constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Tomemos então este exemplo do costume:\n",
    "\n",
    "<img src=\"images/class_2.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a expressão anterior, após treinadas as probabilidades, ver para um dado registo de teste X, qual a classe que maximiza $P(X\\space|\\space C_i)P(C_i)$ (isto é, qual a probilidade posterior $P(C_i|X)$ que fica maximizada)\n",
    "\n",
    "Em que neste caso  $X$ é definido por ```{Home Owner, Marital Status,Annual Income}```, mas vamos chamar aos atributos  com os valores $x_1,x_2,....x_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, é necessário perceber qual das duas probabilidades posteriores é maior:\n",
    "\n",
    "\n",
    "- $P(x_1,x_2,....x_n \\space|\\space Yes) P(Yes)$\n",
    "- $P(x_1,x_2,....x_n\\space | \\space No )P(No)$\n",
    "\n",
    "---\n",
    "\n",
    "### Independencia condicional\n",
    "\n",
    "Se $P(X|Y,Z) = P(X|Y)P(X|Z)$, ou seja, que a relação condicional de Y e Z tem com X não são afectadas entre si. \n",
    "\n",
    "> Quer isto dizer no nosso exemplo que o estado civil, por exemplo, de nada altera a forma como o atributo \"Home Owner\" influencia a probabilidade de default e vice versa. Ou seja, um home owner influencia a probabilidade de default, da mesma forma para casados, solteiros, etc.\n",
    "\n",
    "Apesar de se tratar de uma assunção bastante forte, permite encontrar resultados muito bons em vários problemas.\n",
    "\n",
    "---\n",
    "\n",
    "Neste caso a nossa expressção inicial pode ser escrita na forma:\n",
    "\n",
    "$$P(ho, ms, ai \\space|\\space C_i) = P(ho \\space|\\space C_i) P(ms \\space|\\space C_i) P(ai \\space|\\space C_i)$$\n",
    "\n",
    "\n",
    "ou seja, \n",
    "\n",
    "$$C_i: Max\\{ P(C_i)\\prod{P(x_j \\space|\\space C_i)}\\}$$\n",
    "\n",
    "\n",
    "**Como estimar as probabilidades $P(X \\space|\\space C)$ e $P(C)$ a partir dos dados?**\n",
    "\n",
    "Usar as seguintes frequências relativas:\n",
    "\n",
    "\n",
    "$$P(C) = N_c/N$$\n",
    "\n",
    "$$P(x_j \\space|\\space C_i) = A_{ij}/ N_c$$\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $N_c$ é o número de registos de uma dada classe na amostra de treino;\n",
    "- $N$ é o número de registos na amostra de treino;\n",
    "- $A_{ij}$ é o número de registos da classe i que tem a propriedade j;\n",
    "\n",
    "---\n",
    "\n",
    "No caso de atributos contínuos:\n",
    "\n",
    "- Criar bins por intervalos\n",
    "    - um valor ordinal por bin\n",
    "    - **viola a suposição de independência**\n",
    "    \n",
    "    \n",
    "- Divisão de duas vias: ```(A < v) ou (A > v)```\n",
    "    - escolher apenas uma das duas divisões como novo atributo\n",
    "    \n",
    "    \n",
    "\n",
    "- Assumir atributo obedece certa distribuição de probabilidade\n",
    "    - Normalmente, é assumida a distribuição normal\n",
    "    - Usar dados para estimar parâmetros de distribuição (média e desvio padrão no caso de distribuição normal)\n",
    "    - Uma vez que a distribuição de probabilidade é conhecida, podemos usá-la para estimar a probabilidade condicional $P (x_j \\space|\\space C_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/class_2.png\" style=\"width:40%\"/>\n",
    "\n",
    "Exemplos de Cálculos:\n",
    "\n",
    "\n",
    "- $P($Married|No$) = 4/7$\n",
    "\n",
    "\n",
    "- $P($Refund=Yes|Yes$)=0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propriedades Naif Bayes  \n",
    "\n",
    "BOM \n",
    "- Robusto para pontos de ruído isolados\n",
    "- Trata valores em falta ignorando esse registo durante os cálculos de estimativa de probabilidade\n",
    "- Robusto para atributos irrelevantes \n",
    "\n",
    "MAU\n",
    "\n",
    "- A suposição de independência pode não aguentar para alguns atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sklearn \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_train: 0.961904761905\n",
      "score_test: 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "Iris = datasets.load_iris()\n",
    "from sklearn import datasets, tree, model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "[features_train, features_test, classes_train, classes_test] = model_selection.train_test_split(Iris.data, Iris.target, test_size=0.30)\n",
    "y_pred = gnb.fit(features_train, classes_train)\n",
    "\n",
    "score_train = gnb.score(features_train, classes_train)\n",
    "score_test = gnb.score(features_test, classes_test)\n",
    "\n",
    "print(\"score_train:\", score_train)\n",
    "print(\"score_test:\", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bayes.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### TPC 3\n",
    "\n",
    "- Usando sklearn correr os métodos Decision Tree, Random Forrest e Logistic Regression para o dataset Digits\n",
    "\n",
    "- Usar o training set para executar o treino do modelo.\n",
    "\n",
    "- Comparar o erro obtido em cada método, para o testset e para o training set e expecificar se existe overfitting ou underfitting (Variance ou Bias)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
