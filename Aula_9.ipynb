{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 9 -  Estimation Models \n",
    "\n",
    "## Regression Models - General Linear Models\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como funcionam os modelos de estimação: \n",
    "\n",
    "Considere-se o seguinte dataset com empréstimos. A Classe define se o pedido de empréstimo vai ou não entrar em default (não pagar).\n",
    "\n",
    "<img src=\"images/class_2.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pergunta a que qualquer modelo de classificação (ou regressão) deve responder é:\n",
    "\n",
    "> Conhecendo vários exemplares com a respectiva classificação (train_data ,train_target) quais são as classes de exemplares ainda desconhecidos?\n",
    "\n",
    "<img src=\"images/classification.png\" style=\"width:60%\"/>\n",
    "\n",
    "> Um modelo de Regressão com estes mesmos dados poderia estudar por exemplo qual o rendimento de uma pessoa, mediante um conjunto relevante de Features. Neste caso, os dados de Annual Income seriam o **target**, e as features (talvez insuficientes e até desajustados ao problema) seriam os restantes campos. \n",
    "\n",
    "> **Mas o modelo de estimação seria semelhante.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Métodos de regressão que vamos estudar:\n",
    "\n",
    "[ESTA AULA]\n",
    "\n",
    "- Linear Models for Regression\n",
    "- Generelized Linear Models for Regression\n",
    "\n",
    "----\n",
    "\n",
    "## Métodos de classificação que vamos estudar:\n",
    "\n",
    "[PROXIMAS AULAS]\n",
    "\n",
    "- Linear Model for Classification\n",
    "- Árvores de Decisão;\n",
    "\n",
    "[PROXIMAS AULAS]\n",
    "\n",
    "- Random Forrest;\n",
    "- Naive Bays (Bayesian Methods)\n",
    "- KNN\n",
    "- Kernel Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexto Geral da Regressão\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Considere-se que temos um conjunto de $N$ registos, cada um chamado de $r_i$. Cada registo é composto por $D$ features :\n",
    "\n",
    "$$r_i = \\{(x_1)_i, (x_2)_i,(x_3)_i..., (x_D)_i\\}$$\n",
    "\n",
    "Para cada $r_i$ existe um valor para o seu target $y_i$, pertencente a $\\mathbb{R}$ (números reais).\n",
    "\n",
    "Se quisermos criar um modelo que estime o target de um registo consoante as suas features, podemos genericamente defini-lo como:\n",
    "\n",
    "$$ M(r) =\\hat{y}$$\n",
    "\n",
    "onde $\\hat{y}_i$ é a estimativa de $y_i$ feita pelo nosso modelo $M$. Este modelo terá que ter algum tipo de parametros para que se possa ajustar aos dados. Vamos chamar a esse conjunto de parâmetros (que para já não sabemos os seus valores nem quantos são) de $w = {w_0, w_1, w_2,...}$:\n",
    "\n",
    "\n",
    "$$ M(w, r) =\\hat{y}$$\n",
    "\n",
    "Por exemplo, $M$ pode ser dado por\n",
    "\n",
    "$$M = w_0 + w_1x_1+w_2x_2 +... $$\n",
    "\n",
    "e a sua o aplicação a um novo $r_k  = [x_1, x_2, ... x_m]$ nos dará uma estimativa $\\hat{y}_k$.\n",
    "\n",
    "---\n",
    "\n",
    "Podemos dizer que um modelo estimador que funcione bem deve minimizar a soma de todos os erros encontrados entre o modelo e as features reais. Ou seja, minimizar a soma das diferenças entre os verdadeiros valores do target ($y_i$) e as estimativas encotradas pelo modelo ($M(r_i) = \\hat{y}_i$). \n",
    "\n",
    "Escrevendo isto numa formução matemática esta função a minimizar, chamada de função de erro do nosso modelo, obtemos:\n",
    "\n",
    "$$\\sum_i^N{|\\hat{y}_i-y_i|}$$\n",
    "\n",
    "Se em vez de um módulo usarmos o quadrado do erro de cada registo, obtemos\n",
    "\n",
    "$$Cost = \\sum_i^N{(\\Delta{e})^2} = \\sum_i^N{(\\hat{y}_i-y_i)^2}$$\n",
    "\n",
    "Que é vulgarmente chamada de *Cost Function* (função de custo).\n",
    "\n",
    "---\n",
    "\n",
    "Finalmente podemos escrever o nosso problema de forma genérica, ou seja, para qualquer que seja o nosso estimador $M$:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\text{min}} \\space\\space \\frac{1}{2}\\sum_i^N{(M(w,r_i)-y_i)^2}$$\n",
    "\n",
    "Ou seja, o nosso objectivo é \n",
    "\n",
    "> **Determinar os $w$ que melhor adaptam o resultado modelo $M$ aos dados de target $y_i$. Os $w$ funcionam como pesos que nos permitem adaptar o modelo $M$ aos dados recebidos ($y_i$).**\n",
    "\n",
    "O valor 1/2 foi adicionado para simplificar as contas mais à frente, mas nada muda no problema de optimzização (porque?)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "---\n",
    "Vamos começar por avaliar o modelo mais simples de Regressão: a regressão linear com uma só feature e com um modelo tipo $y = mx+b$. \n",
    "\n",
    "> No entanto, é importante saber que a regressão diz-se linear porque é linear em relação aos parâmetros $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de Regressão Linear considera que existe a dependência linear das diversas features, ou seja, do tipo $M(w,r) = wr$. Vamos considerar, para simplificar que temos 1 só feature ou seja, $r_i = x_i$. O nosso modelo $M$, fica:\n",
    "\n",
    "$$ \\hat{y} = M(w, r) = w_0 + w_1x$$\n",
    "\n",
    "> Que com certeza que vos faz lembrar o famoso $y=mx+b$. \n",
    "\n",
    "Estamos a dizer que o nosso modelo M estima $\\hat{y}_i$ a partir de uma relação linear com as features, neste caso, $x$. Como foi dito, para determinarmos os $w's$ temos que resolver o problema:\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\text{min}} \\space\\space \\frac{1}{2}\\sum_i^N{(M(w,r_i)-y_i)^2}$$ \n",
    "\n",
    "onde, neste caso, temos\n",
    "\n",
    "$$\\hat{w} = \\underset{w}{\\text{min}} \\space\\space \\frac{1}{2}\\sum_i^N{(w_0 + w_1x_i-y_i)^2}$$ \n",
    "\n",
    "Um problema de optimização deste deve ser resolvido procurando o ponto em que a derivada em função dos parametros para os parametros $w_0$ e $w_1$ é nula. (uma vez que o mínimo será o mínimo dos quadrados). As expressões das derivadas iguais a zero são:\n",
    "\n",
    "$$\\frac{dCost}{dw_0} = 0: \\sum_i^N{(w_0 + w_1x_i - y_i)} = 0$$\n",
    "\n",
    "e\n",
    "\n",
    "$$\\frac{dCost}{dw_1} = 0: \\sum_i^N{(w_0 + w_1x_i - y_i)x_i} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "quer dizer que podemos obter os parâmetros $w_0$ e $w_1$ resolvendo as seguintes equações:\n",
    "\n",
    "$$\\hat{w}_0 = \\mathbb{E}(y)-\\hat{w}_1\\mathbb{E}(x)$$\n",
    "\n",
    "$$\\sum_i^N{(w_0x_i + \\hat{w}_1x_i^2)} = \\sum_i^N{y_ix_i}$$\n",
    "\n",
    "\n",
    "onde $$\\mathbb{E}(x) = 1/N\\sum_i^N{x_i}$$\n",
    "\n",
    "---\n",
    "$$\\hat{w}_0 = \\mathbb{E}(y)-\\hat{w}_1\\mathbb{E}(x)$$\n",
    "\n",
    "$$\\frac{1}{N}(\\sum_i^N{(\\mathbb{E}(y)x_i-\\hat{w}_1\\mathbb{E}(x)x_i + \\hat{w}_1x_i^2)}) = \\frac{1}{N}(\\sum_i^N{y_ix_i})$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{w}_0 = \\mathbb{E}(y)-\\hat{w}_1\\mathbb{E}(x)$$\n",
    "\n",
    "$$\\hat{w}_1 = \\frac{\\mathbb{E}(yx)-\\mathbb{E}(y)\\mathbb{E}(x)}{\\mathbb{E}(x^2)-\\mathbb{E}(x)^2} $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por curiosidade, esta última expressão pode ser escrita da forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\hat{w}_1 = \\frac{COV(y,x)}{VAR(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mas que expressão devemos usar???\n",
    "\n",
    "---\n",
    "\n",
    "As expressões anteriores, apesar de correctas são complexas. Existe uma forma mais simples de resolver este problema: usando a descrição matricial do problema.\n",
    "\n",
    "$$ \\hat{y} = M(w, r) = \\textbf{r}^T\\textbf{w}$$\n",
    "\n",
    "\n",
    "Matricialmente, podemos escrever o sistema da seguinte forma:\n",
    "\n",
    "$$Cost = \\frac{1}{2} ||\\textbf{Rw}-\\textbf{y}||^2 = \\frac{1}{2} (\\textbf{Rw}-\\textbf{y})^T(\\textbf{Rw}-\\textbf{y})$$\n",
    "\n",
    "ou seja, derivando em ordem a w, e igualando a 0 obtemos:\n",
    "\n",
    "$$(\\textbf{R}(\\textbf{Rw}-\\textbf{y})^T + \\textbf{R}^T(\\textbf{Rw}-\\textbf{y}))$$\n",
    "\n",
    "$$\\textbf{R}^T(\\textbf{Rw}-\\textbf{y}) = 0$$\n",
    "\n",
    "$$\\textbf{R}^T\\textbf{R w} = \\textbf{R}^T\\textbf{y}$$\n",
    "\n",
    "\n",
    "\n",
    "O resultado corresponde a:\n",
    "\n",
    "$$\\textbf{w} = (\\textbf{R}^T\\textbf{R})^{-1}\\textbf{R}^T\\textbf{y}$$\n",
    "\n",
    "Vamos verificar como aplicar esta expressão para os w's com um exemplo:\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\textbf{r}^T (\\textbf{R}^T \\textbf{R})^{-1} \\textbf{R}^T \\textbf{y} $$\n",
    "\n",
    "\n",
    "\n",
    "> **A solução é única - quer isto dizer que é muito rápida de determinar - o problema é que não funciona para relações muito complexas...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exemplo Regressão Linear\n",
    "\n",
    "Vamos tomar o seguinte exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-87a5e0888f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2283\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2285\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-107>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \"\"\"\n\u001b[1;32m   3340\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3341\u001b[0;31m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \"\"\"\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "x = (np.random.random([100,1])*100).reshape(-1,1)\n",
    "m = 5;\n",
    "b = 40\n",
    "y= m*(x) + b\n",
    "y = y+y.mean()*0.2*(np.random.random((y.shape))-0.5)\n",
    "\n",
    "plt.plot(x,y,\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.hstack([np.ones((x.shape)).reshape(-1,1),x])\n",
    "\n",
    "#w_fit = (R^TR)^{-1}R^Ty\n",
    "y_intermediate = np.matmul(R.T,y)\n",
    "w_fit = np.matmul(np.linalg.inv(np.matmul(R.T,R)),y_intermediate)\n",
    "\n",
    "y_fit = w_fit[0]+w_fit[1]*x\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y,\".\")\n",
    "plt.plot(x,y_fit,\"-\")\n",
    "plt.show()\n",
    "\n",
    "print(\"----\")\n",
    "print(\"m = \", m)\n",
    "print(\"w_1 = \", w_fit[1])\n",
    "print(\"b = \", b)\n",
    "print(\"w_0 = \", w_fit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora usando o sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "lr = lm.LinearRegression()\n",
    "data=lr.fit(x,y)\n",
    "w_fit = []\n",
    "w_fit.append(data.intercept_)\n",
    "w_fit.append(data.coef_)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"m = \", m)\n",
    "print(\"w_1 = \", w_fit[1])\n",
    "print(\"b = \", b)\n",
    "print(\"w_0 = \", w_fit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Multi-Linear Regression\n",
    "---\n",
    "\n",
    "O modelo de Regressão Linear considera que existe numa primeira aproximação, uma dependência linear das diversas features. Mas como temos $D$ features, a nossa nova função $M$, fica:\n",
    "\n",
    "$$ M(w, r) = w_0 + w_1x_1 + w_2x_2 + ... + w_Dx_D$$\n",
    "\n",
    "ou seja, linear para cada feature.\n",
    "\n",
    "Vamos ver um exemplo com $D=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random_sample([1000,2])*20-10\n",
    "x[:10,:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_with_offset = np.hstack([np.ones((x.shape[0],1)),x])\n",
    "\n",
    "w = [10,5,-3]#, -8, 1, -6,12,3,9,2]\n",
    "\n",
    "y = np.dot(x_with_offset,w)\n",
    "\n",
    "# 30% Error!!!\n",
    "y = y+.3*y.mean()*(np.random.random(y.shape)-0.5)\n",
    "#print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x[:,0],x[:,1], y, c=\"black\", s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lm.LinearRegression()\n",
    "data=lr.fit(x,y)\n",
    "w_fit = np.array(data.coef_)\n",
    "w_fit = np.concatenate([np.array([data.intercept_]),w_fit]).reshape(-1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----\")\n",
    "print(\"w original = \", w)\n",
    "print(\"w_fit = \", w_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = w_fit[0,0] + np.matmul(w_fit[:,1:],x.T)\n",
    "y_fit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos visualizar o fit que encontrámos:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.arange(-10, 10, 0.25)\n",
    "Y = np.arange(-10, 10, 0.25)\n",
    "X,Y = np.meshgrid(X, Y)\n",
    "S = np.concatenate([X.reshape(-1,1),Y.reshape(-1,1)], axis=1)\n",
    "S.shape\n",
    "\n",
    "Y_FIT = w_fit[0,0] + np.matmul(w_fit[:,1:],S.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x[:,0],x[:,1], y, c=\"black\", s=1)\n",
    "surf = ax.plot_wireframe(X, Y, Y_FIT.reshape(X.shape), rstride=3, cstride=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como medir o erro?\n",
    "\n",
    "\n",
    "O erro pode medir-se com o valor da função de custo:\n",
    "\n",
    "$$Cost = \\underset{w}{\\text{min}} \\space\\space \\frac{1}{2}\\sum_i^N{(\\hat{y}_i-y_i)^2}$$ \n",
    "\n",
    "ou seja, calculando de facto a soma dos erros para os diversos pontos de treino ou mesmo para um conjunto de pontos de teste.\n",
    "\n",
    "Neste último caso teríamos, usando a descrição matricial, por exemplo o erro em fase de treino dado por - Root Mean Square Error:\n",
    "\n",
    "\n",
    "$$RMSE_{train} = \\space\\space \\sqrt{\\frac{{||\\textbf{R} \\textbf{w}-\\textbf{y}||^2}}{N}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = (((y-y_fit)**2).sum()/y.shape[0])**0.5\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# General Linear Regression (Regressão Linear Geral)\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar um polinómio de 3ª ordem para fazer fit de uma função coseno, ou seja, o modelo $M$ é dado por algo como:\n",
    "\n",
    "\n",
    "$$ M(w, r) = w_0 + w_1x_1 + ... + w_2(x_1)^2 + ... +  w_j(x_D)^2 + w_{j+1}(x_1)^3 + ...  $$\n",
    "\n",
    "$$ M(w, z) = w_0 + w_1z_1 + w_3z_2 + ... +  w_jz_3 + w_{j+1}z_4 + ...  $$\n",
    "\n",
    "repare-se que a lógica linear em termos dos parâmetros $w$ se mantém.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random_sample([100,1])*4-2\n",
    "\n",
    "y= 3*(np.cos(x)+0.0*(np.random.random((x.shape[0],1))-0.5)).reshape(-1,1)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x,y,\".\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ce3746e84bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm' is not defined"
     ]
    }
   ],
   "source": [
    "lr = lm.LinearRegression()\n",
    "\n",
    "x_2 = x**2\n",
    "x_3 = x**3\n",
    "X = np.concatenate([x,x_2,x_3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit (X,y)\n",
    "lr.coef_\n",
    "\n",
    "\n",
    "y_fit = np.matmul(X, lr.coef_.T) + lr.intercept_\n",
    "\n",
    "A = [(x_,y_f_, y_) for x_,y_f_,y_ in zip(x,y_fit,y)]\n",
    "\n",
    "A = sorted(A, key=lambda a: a[0])\n",
    "x = [a[0] for a in A]\n",
    "y = [a[2] for a in A]\n",
    "y_fit = [a[1] for a in A]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x,y,\".\")\n",
    "\n",
    "plt.plot(x,y_fit,\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - TPC XXX\n",
    "\n",
    "\n",
    "Considere o seguinte um data set com 1000 registos com 2 features ($x_1$ e $x_2$), dado por X (definido em baixo) e respectivos target values dados por \n",
    "\n",
    "$y = 3cos(x_0)cos(5x_1) + noise(30 \\%)$\n",
    "\n",
    "Encontre o melhor fit (y_fit) com polinómios até à 6 ordem.\n",
    "Mostre o valor do erro do fit (RMSE) do seu modelo $M$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random_sample([1000,2])*5-1\n",
    "X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= (3*(np.cos(X[:,0]))*np.cos(2*X[:,1])).reshape(-1,1)\n",
    "\n",
    "# add noise\n",
    "y = y+0.03*abs(y).mean()*np.random.random((X.shape[0],1)).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize o seguinte código para desenhar x, y, e y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eae57f2d51de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits'"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "X_ = np.arange(0, 5, 0.1)\n",
    "Y_ = np.arange(0, 5, 0.1)\n",
    "X_, Y_ = np.meshgrid(X_, Y_)\n",
    "\n",
    "y= (3*(np.cos(X_))*np.cos(2*Y_))\n",
    "\n",
    "# add noise\n",
    "y = y+0.3*abs(y).max()*np.random.random(y.shape)\n",
    "Z = y\n",
    "\n",
    "X_.shape\n",
    "Z.shape\n",
    "\n",
    "\n",
    "surf = ax.plot_surface(X_, Y_, Z)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
